<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <a href="projects.html">Projects</a>
    </nav>

    <!-- Main Content Area -->
    <main>
        <h1>Projects</h1>
        
        
        <section>
    <h2>Project 0: Token-by-Token Text Generation using GPT-2</h2>
    <p>
        This project demonstrates how large language models like GPT-2 generate text one token at a time. 
        I explored how the model calculates the probability distribution for the next token based on the input 
        sequence, then selected the most likely token to extend the text step-by-step. The project also showcases 
        the difference between manual generation and the built-in generate() method provided by HuggingFace's 
        Transformers library.
    </p>
    <p>
        Key concepts covered: Subword Tokenization (Byte Pair Encoding in GPT-2), Logits to Probabilities with Softmax, Argmax-based sampling, Autoregressive Text Generation,
        Comparison between manual generation and generate() method
        
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Tokenization/Tokenization.ipynb">Github Link</a>
    </p>
        </section>

        <section>
    <h2>Project 1: Classification of Handwritten Digits using MLP</h2>
    <p>
        In this project, I trained a <strong>Multi-Layer Perceptron (MLP)</strong> to classify handwritten digits from the 
        <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank">MNIST dataset</a>. 
        The MNIST dataset consists of 28×28 grayscale images of digits from 0 to 9, and the goal was to correctly classify each image into one of 10 digit classes.
        Using the <code>MLPClassifier</code> from <code>scikit-learn</code>, I built and trained a simple neural network with one hidden layer. 
        The model was evaluated on a separate test set and achieved high classification accuracy. The project also includes visualization of predictions, comparing true vs. predicted labels, with color-coded highlights to spot misclassified examples—an important step for model debugging.

    </p>
    <p>
        Key concepts covered: Loading and preprocessing the MNIST dataset using <code>fetch_openml</code>, Training a neural network using <code>MLPClassifier</code>,
            Evaluating classification performance on train and test sets, Visualizing correct and incorrect predictions
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/MLP%20Classification/MLP_Classification_of_Handwritten_Digits.ipynb">GitHub Link</a>
    </p>
</section>

<section>
    <h2>Project 2: Hands-On Introduction to Neural Networks with PyTorch and HuggingFace</h2>
    <p>
        In this project, I built a solid foundation in neural networks by working hands-on with two widely used deep learning libraries: 
        <strong>PyTorch</strong> and <strong>HuggingFace Transformers</strong>. The goal was to develop a deep understanding of how to 
        construct, train, and apply neural networks using both low-level building blocks and high-level pre-trained models. The project consists of two main parts:
    <ul>
        <li>
            <strong>PyTorch Fundamentals:</strong> I created tensors, constructed a custom Multi-Layer Perceptron (MLP) using <code>torch.nn</code>, 
            defined a loss function and optimizer, and implemented a training loop from scratch. These steps demonstrated the full workflow 
            of training a neural network.
        </li>
        <li>
            <strong>HuggingFace Transformers:</strong> I used a pre-trained sentiment analysis model 
            (<code>distilbert-base-uncased-finetuned-sst-2-english</code>) to classify movie reviews. I also loaded and evaluated real-world 
            text data using the HuggingFace <code>datasets</code> library.
        </li>
    </ul>
    </p>
    <p>
        Key concepts covered: Creating and manipulating tensors on CPU/GPU with PyTorch, Building a feedforward neural network (MLP) using PyTorch modules,
            Training a model using forward pass, loss computation, backpropagation, and optimization,
            Using HuggingFace Transformers for zero-shot sentiment classification,
            Loading and iterating over the IMDb dataset using the HuggingFace datasets API
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Pytorch%20and%20Hugging%20Face%20Exploration/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>



       <section>
    <h2>Project 3: Transfer Learning for Fashion Image Classification using MobileNetV3</h2>
    <p>
        In this project, I explored <strong>transfer learning</strong> using a pre-trained <code>MobileNetV3</code> architecture to classify 
        fashion items from the <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank">Fashion-MNIST</a> dataset. 
        This dataset consists of 28×28 grayscale images of clothing items like shirts, sneakers, and coats, serving as a drop-in replacement for MNIST digits.
        <strong>MobileNetV3</strong> is a lightweight convolutional neural network designed for efficiency on mobile and edge devices. In this project, 
        I fine-tuned the model by replacing the final classification layer to suit our 10 fashion categories and freezing the rest of the network. This 
        approach significantly reduces training time while leveraging powerful pre-learned image representations.
    </p>
    <p>
        Key Concepts covered: Transfer learning with pre-trained CNNs, Modifying classifier layers for custom tasks,
        Freezing and unfreezing network parameters, Resizing grayscale images to fit color CNN input formats, Efficient training with only a small subset of parameters,
        Model evaluation and visualization of classification results
    
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Transfer%20Learning%20using%20MobileNet/Transfer%20learning%20using%20MobileNetV3.ipynb">GitHub Link</a>
    </p>
</section>


        <section>
    <h2>Project 4: Prompt-Based Spam Email Classifier using a Foundation Model</h2>
    <p>
        In this project, I built a spam email classifier using a <strong>foundation model</strong> accessed through prompting—without any training or fine-tuning. 
        The goal was to showcase how a pre-trained large language model (LLM) can be used "as-is" to solve classification problems through smart prompt engineering.
        I used the <code>sms_spam</code> dataset from HuggingFace, which contains real SMS messages labeled as SPAM or NOT SPAM. The model was prompted to classify a batch of messages based on their content, and its output was parsed and evaluated against ground truth labels.
    </p>
    <p>
        The project was carried out in two stages:
        <ul>
            <li><strong>Zero-shot classification:</strong> The LLM was asked to classify SMS messages with no prior examples.</li>
            <li><strong>Few-shot classification:</strong> The prompt was augmented with a few labeled examples before asking the model to classify new, unlabeled messages. This improved its accuracy significantly.</li>
        </ul>
    </p>

    <p>
        Key concepts covered: Loading and preparing a labeled dataset from HuggingFace, Constructing zero-shot and few-shot prompts for classification tasks,
            Parsing LLM JSON responses and evaluating prediction accuracy, Using LLMs for real-world tasks without fine-tuning
        
    </p>

    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Prompt%20Based%20Spam%20Email%20Classifier%20using%20a%20Foundation%20Model/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>


        <section></section>
            <h2>Project 5: K-Nearest Neighbors Kaggle Competition </h2>
            <p>
                In this project, I implemented a K-Nearest Neighbors (KNN) model from scratch to predict customer 
                churn for a bank. The goal was to identify customers who are likely to leave the bank based on historical 
                data and submit the predictions in a mini Kaggle competition.
                <!--<img src="images/Kaggle.png" alt="Kaggle Score" width="500"> -->


            <p>
                <a href="https://github.com/Swatkats13/jains-assigment-5.git">Github Link</a>
            </p>
            
        </section>


        <section></section>
        <h2>Project 6: Linear Regression </h2>
        <p>
            In this project, I explore the impact of changing parameters on linear regression. 
            The goal was to create an interactive webpage to demonstrate how modifying these parameters affects regression results, 
            especially when there is no actual relationship between X and Y. By tweaking these settings, we can observe how randomness 
            can influence the slope and intercept in a regression model.


        <p>


            <a href="https://github.com/Swatkats13/jains-assignment-6.git">Github Link</a>
        </p>
        

        <video width="640" height="360" controls>
            <source src="videos/Regression.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>


    </section>

    <h2>Project 7: Hypothesis Testing and Confidence Intervals in Linear Regression </h2>
    <p>
        In this project, I extended my previous work from project 6 to include hypothesis 
        testing and confidence intervals through simulations. I enhanced the interactive webpage to 
        allow users to perform hypothesis tests on the slope or intercept of the regression line and 
        generate confidence intervals based on simulations.


    <p>


        <a href="https://github.com/Swatkats13/jains-assignment-7.git">Github Link</a>
    </p>
    

    <video width="640" height="360" controls>
        <source src="videos/Regression2.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>


</section>


<h2>Project 8: Logistic Regression </h2>
<p>
    In this project, I explored the effect of shifting clusters in a dataset 
    on the parameters of a logistic regression model.

<p>


    <a href="https://github.com/Swatkats13/jains-assignment-8.git">Github Link</a>
</p>


<video width="640" height="360" controls>
    <source src="videos/LogReg.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

</section>

<h2>Project 9: Neural Networks </h2>
<p>
    In this project, I implemented and analyzed a simple neural network by visualizing its learned features,
    decision boundary and gradients. The goal was to develop a deeper understanding of how a Feedforward Neural Network with 
    one hidden layer operates and represents the input space during learning.

<p>

    <a href="https://github.com/Swatkats13/jains-assignment-9.git">Github Link</a>
</p>

<video width="640" height="360" controls>
    <source src="videos/nn.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

</section>

<h2>Project 10: Image Search </h2>
<p>
    In this project, the goal was to implement a simplified version of Google Image Search.
<p>

    <a href="https://github.com/Swatkats13/jains-assignment-10.git">Github Link</a>
</p>

<video width="640" height="360" controls>
    <source src="videos/ImageSearch.mov" type="video/mp4">
    Your browser does not support the video tag.
</video>


</section>
       
    </main>
</body>
</html>
