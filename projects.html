<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <a href="projects.html">Projects</a>
    </nav>

    <!-- Main Content Area -->
    <main>
        <h1>Projects</h1>
        
        
        <section>
    <h2>Project 0: Token-by-Token Text Generation using GPT-2</h2>
    <p>
        This project demonstrates how large language models like GPT-2 generate text one token at a time. 
        I explored how the model calculates the probability distribution for the next token based on the input 
        sequence, then selected the most likely token to extend the text step-by-step. The project also showcases 
        the difference between manual generation and the built-in generate() method provided by HuggingFace's 
        Transformers library.
    </p>
    <p>
        Key concepts covered: Subword Tokenization (Byte Pair Encoding in GPT-2), Logits to Probabilities with Softmax, Argmax-based sampling, Autoregressive Text Generation,
        Comparison between manual generation and generate() method
        
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Tokenization/Tokenization.ipynb">Github Link</a>
    </p>
        </section>

        <section>
    <h2>Project 1: Classification of Handwritten Digits using MLP</h2>
    <p>
        In this project, I trained a <strong>Multi-Layer Perceptron (MLP)</strong> to classify handwritten digits from the 
        <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank">MNIST dataset</a>. 
        The MNIST dataset consists of 28×28 grayscale images of digits from 0 to 9, and the goal was to correctly classify each image into one of 10 digit classes.
        Using the <code>MLPClassifier</code> from <code>scikit-learn</code>, I built and trained a simple neural network with one hidden layer. 
        The model was evaluated on a separate test set and achieved high classification accuracy. The project also includes visualization of predictions, comparing true vs. predicted labels, with color-coded highlights to spot misclassified examples—an important step for model debugging.

    </p>
    <p>
        Key concepts covered: Loading and preprocessing the MNIST dataset using <code>fetch_openml</code>, Training a neural network using <code>MLPClassifier</code>,
            Evaluating classification performance on train and test sets, Visualizing correct and incorrect predictions
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/MLP%20Classification/MLP_Classification_of_Handwritten_Digits.ipynb">GitHub Link</a>
    </p>
</section>

<section>
    <h2>Project 2: Hands-On Introduction to Neural Networks with PyTorch and HuggingFace</h2>
    <p>
        In this project, I built a solid foundation in neural networks by working hands-on with two widely used deep learning libraries: 
        <strong>PyTorch</strong> and <strong>HuggingFace Transformers</strong>. The goal was to develop a deep understanding of how to 
        construct, train, and apply neural networks using both low-level building blocks and high-level pre-trained models. The project consists of two main parts:
    <ul>
        <li>
            <strong>PyTorch Fundamentals:</strong> I created tensors, constructed a custom Multi-Layer Perceptron (MLP) using <code>torch.nn</code>, 
            defined a loss function and optimizer, and implemented a training loop from scratch. These steps demonstrated the full workflow 
            of training a neural network.
        </li>
        <li>
            <strong>HuggingFace Transformers:</strong> I used a pre-trained sentiment analysis model 
            (<code>distilbert-base-uncased-finetuned-sst-2-english</code>) to classify movie reviews. I also loaded and evaluated real-world 
            text data using the HuggingFace <code>datasets</code> library.
        </li>
    </ul>
    </p>
    <p>
        Key concepts covered: Creating and manipulating tensors on CPU/GPU with PyTorch, Building a feedforward neural network (MLP) using PyTorch modules,
            Training a model using forward pass, loss computation, backpropagation, and optimization,
            Using HuggingFace Transformers for zero-shot sentiment classification,
            Loading and iterating over the IMDb dataset using the HuggingFace datasets API
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Pytorch%20and%20Hugging%20Face%20Exploration/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>



       <section>
    <h2>Project 3: Transfer Learning for Fashion Image Classification using MobileNetV3</h2>
    <p>
        In this project, I explored <strong>transfer learning</strong> using a pre-trained <code>MobileNetV3</code> architecture to classify 
        fashion items from the <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank">Fashion-MNIST</a> dataset. 
        This dataset consists of 28×28 grayscale images of clothing items like shirts, sneakers, and coats, serving as a drop-in replacement for MNIST digits.
        <strong>MobileNetV3</strong> is a lightweight convolutional neural network designed for efficiency on mobile and edge devices. In this project, 
        I fine-tuned the model by replacing the final classification layer to suit our 10 fashion categories and freezing the rest of the network. This 
        approach significantly reduces training time while leveraging powerful pre-learned image representations.
    </p>
    <p>
        Key Concepts covered: Transfer learning with pre-trained CNNs, Modifying classifier layers for custom tasks,
        Freezing and unfreezing network parameters, Resizing grayscale images to fit color CNN input formats, Efficient training with only a small subset of parameters,
        Model evaluation and visualization of classification results
    
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Transfer%20Learning%20using%20MobileNet/Transfer%20learning%20using%20MobileNetV3.ipynb">GitHub Link</a>
    </p>
</section>


        <section>
    <h2>Project 4: Prompt-Based Spam Email Classifier using a Foundation Model - Prompt Based</h2>
    <p>
        In this project, I built a spam email classifier using a <strong>foundation model</strong> accessed through prompting—without any training or fine-tuning. 
        The goal was to showcase how a pre-trained large language model (LLM) can be used "as-is" to solve classification problems through smart prompt engineering.
        I used the <code>sms_spam</code> dataset from HuggingFace, which contains real SMS messages labeled as SPAM or NOT SPAM. The model was prompted to classify a batch of messages based on their content, and its output was parsed and evaluated against ground truth labels.
    </p>
    <p>
        The project was carried out in two stages:
        <ul>
            <li><strong>Zero-shot classification:</strong> The LLM was asked to classify SMS messages with no prior examples.</li>
            <li><strong>Few-shot classification:</strong> The prompt was augmented with a few labeled examples before asking the model to classify new, unlabeled messages. This improved its accuracy significantly.</li>
        </ul>
    </p>

    <p>
        Key concepts covered: Loading and preparing a labeled dataset from HuggingFace, Constructing zero-shot and few-shot prompts for classification tasks,
            Parsing LLM JSON responses and evaluating prediction accuracy, Using LLMs for real-world tasks without fine-tuning
        
    </p>

    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Prompt%20Based%20Spam%20Email%20Classifier%20using%20a%20Foundation%20Model/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>


        <section>
    <h2>Project 5: Sentiment Classification Using DistilBERT on IMDB Dataset - Linear Probing</h2>
    <p>
        In this project, I fine-tuned a <strong>DistilBERT</strong> model (a lightweight version of BERT) using the 
        <a href="https://huggingface.co/datasets/imdb" target="_blank">IMDB movie review dataset</a> to classify movie reviews 
        as either <em>positive</em> or <em>negative</em>. This task showcases the power of <strong>transformer-based models</strong> 
        in real-world natural language processing (NLP) applications. I used the <strong>Hugging Face Transformers</strong> library to tokenize the dataset, set up the classification model, 
        and train it using the high-level <code>Trainer</code> API. To accelerate training and preserve compute, I froze the base 
        DistilBERT model and trained only the classification head. 
        This project demonstrates end-to-end fine-tuning of a foundation model using minimal code, while still maintaining flexibility 
        and transparency in each training step.
    </p>

    <p>
        Key steps included:
        
            Preprocessing the dataset using <code>AutoTokenizer</code> with padding and truncation,
            Loading a pre-trained <code>distilbert-base-uncased</code> model and configuring it for binary classification,
            Training using Hugging Face's <code>Trainer</code> and evaluating accuracy on the test set,
            Analyzing model predictions and identifying misclassified examples for error analysis
        
    </p>

    

    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Bert%20Sentiment%20Classifier%20-%20Linear%20Probing/BERT_Sentiment_Classifier.ipynb">GitHub Link</a>
    </p>
</section>



       <section>
    <h2>Project 6: Fine-Tuning DistilBERT for SMS Spam Classification - Full Fine Tuning</h2>
    <p>
        In this project, I built a binary text classifier by <strong>fully fine-tuning a DistilBERT model</strong> using the 
        <a href="https://huggingface.co/datasets/sms_spam" target="_blank">SMS Spam dataset</a> from Hugging Face. The task involved 
        classifying short text messages as either <em>spam</em> or <em>not spam</em>, a practical application of NLP in communication security.
        Unlike prompt-based or head-only training, this project involved **unfreezing all layers** of the pre-trained transformer model, 
        allowing the entire DistilBERT architecture to adapt to the task through backpropagation. This is known as <strong>full fine-tuning</strong>, 
        which typically results in better performance on downstream tasks with enough data. This project demonstrates a robust end-to-end training pipeline for a real-world classification task using transformer models.
        It also emphasizes the benefits and trade-offs of full fine-tuning compared to parameter-efficient alternatives.
    </p>

    

    <p>
        Key steps included:
        
            Loading and splitting the SMS Spam dataset into training and testing subsets,
            Tokenizing text data using the <code>AutoTokenizer</code> from Hugging Face,
            Fully fine-tuning <code>distilbert-base-uncased</code> using the <code>Trainer</code> API,
            Evaluating model accuracy on unseen test messages,
            Manually inspecting misclassified examples for qualitative error analysis
        
    </p>


    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Bert%20Spam%20Classifier%20-%20Full%20Fine%20Tuning/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>



    </section>

    <h2>Project 7: Hypothesis Testing and Confidence Intervals in Linear Regression </h2>
    <p>
        In this project, I extended my previous work from project 6 to include hypothesis 
        testing and confidence intervals through simulations. I enhanced the interactive webpage to 
        allow users to perform hypothesis tests on the slope or intercept of the regression line and 
        generate confidence intervals based on simulations.


    <p>


        <a href="https://github.com/Swatkats13/jains-assignment-7.git">Github Link</a>
    </p>
    

    <video width="640" height="360" controls>
        <source src="videos/Regression2.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>


</section>


<h2>Project 8: Logistic Regression </h2>
<p>
    In this project, I explored the effect of shifting clusters in a dataset 
    on the parameters of a logistic regression model.

<p>


    <a href="https://github.com/Swatkats13/jains-assignment-8.git">Github Link</a>
</p>


<video width="640" height="360" controls>
    <source src="videos/LogReg.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

</section>

<h2>Project 9: Neural Networks </h2>
<p>
    In this project, I implemented and analyzed a simple neural network by visualizing its learned features,
    decision boundary and gradients. The goal was to develop a deeper understanding of how a Feedforward Neural Network with 
    one hidden layer operates and represents the input space during learning.

<p>

    <a href="https://github.com/Swatkats13/jains-assignment-9.git">Github Link</a>
</p>

<video width="640" height="360" controls>
    <source src="videos/nn.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

</section>

<h2>Project 10: Image Search </h2>
<p>
    In this project, the goal was to implement a simplified version of Google Image Search.
<p>

    <a href="https://github.com/Swatkats13/jains-assignment-10.git">Github Link</a>
</p>

<video width="640" height="360" controls>
    <source src="videos/ImageSearch.mov" type="video/mp4">
    Your browser does not support the video tag.
</video>


</section>
       
    </main>
</body>
</html>
