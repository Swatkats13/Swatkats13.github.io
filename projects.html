<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <a href="projects.html">Projects</a>
    </nav>

    <!-- Main Content Area -->
    <main>
        <h1>Projects</h1>
        
        
        <section>
    <h2>Project 0: Token-by-Token Text Generation using GPT-2</h2>
    <p>
        This project demonstrates how large language models like GPT-2 generate text one token at a time. 
        I explored how the model calculates the probability distribution for the next token based on the input 
        sequence, then selected the most likely token to extend the text step-by-step. The project also showcases 
        the difference between manual generation and the built-in generate() method provided by HuggingFace's 
        Transformers library.
    </p>
    <p>
        Key concepts covered: Subword Tokenization (Byte Pair Encoding in GPT-2), Logits to Probabilities with Softmax, Argmax-based sampling, Autoregressive Text Generation,
        Comparison between manual generation and generate() method
        
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Tokenization/Tokenization.ipynb">Github Link</a>
    </p>
        </section>

        <section>
    <h2>Project 1: Classification of Handwritten Digits using MLP</h2>
    <p>
        In this project, I trained a <strong>Multi-Layer Perceptron (MLP)</strong> to classify handwritten digits from the 
        <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank">MNIST dataset</a>. 
        The MNIST dataset consists of 28×28 grayscale images of digits from 0 to 9, and the goal was to correctly classify each image into one of 10 digit classes.
        Using the <code>MLPClassifier</code> from <code>scikit-learn</code>, I built and trained a simple neural network with one hidden layer. 
        The model was evaluated on a separate test set and achieved high classification accuracy. The project also includes visualization of predictions, comparing true vs. predicted labels, with color-coded highlights to spot misclassified examples—an important step for model debugging.

    </p>
    <p>
        Key concepts covered: Loading and preprocessing the MNIST dataset using <code>fetch_openml</code>, Training a neural network using <code>MLPClassifier</code>,
            Evaluating classification performance on train and test sets, Visualizing correct and incorrect predictions
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/MLP%20Classification/MLP_Classification_of_Handwritten_Digits.ipynb">GitHub Link</a>
    </p>
</section>

<section>
    <h2>Project 2: Hands-On Introduction to Neural Networks with PyTorch and HuggingFace</h2>
    <p>
        In this project, I built a solid foundation in neural networks by working hands-on with two widely used deep learning libraries: 
        <strong>PyTorch</strong> and <strong>HuggingFace Transformers</strong>. The goal was to develop a deep understanding of how to 
        construct, train, and apply neural networks using both low-level building blocks and high-level pre-trained models. The project consists of two main parts:
    <ul>
        <li>
            <strong>PyTorch Fundamentals:</strong> I created tensors, constructed a custom Multi-Layer Perceptron (MLP) using <code>torch.nn</code>, 
            defined a loss function and optimizer, and implemented a training loop from scratch. These steps demonstrated the full workflow 
            of training a neural network.
        </li>
        <li>
            <strong>HuggingFace Transformers:</strong> I used a pre-trained sentiment analysis model 
            (<code>distilbert-base-uncased-finetuned-sst-2-english</code>) to classify movie reviews. I also loaded and evaluated real-world 
            text data using the HuggingFace <code>datasets</code> library.
        </li>
    </ul>
    </p>
    <p>
        Key concepts covered: Creating and manipulating tensors on CPU/GPU with PyTorch, Building a feedforward neural network (MLP) using PyTorch modules,
            Training a model using forward pass, loss computation, backpropagation, and optimization,
            Using HuggingFace Transformers for zero-shot sentiment classification,
            Loading and iterating over the IMDb dataset using the HuggingFace datasets API
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Pytorch%20and%20Hugging%20Face%20Exploration/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>



       <section>
    <h2>Project 3: Transfer Learning for Fashion Image Classification using MobileNetV3</h2>
    <p>
        In this project, I explored <strong>transfer learning</strong> using a pre-trained <code>MobileNetV3</code> architecture to classify 
        fashion items from the <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank">Fashion-MNIST</a> dataset. 
        This dataset consists of 28×28 grayscale images of clothing items like shirts, sneakers, and coats, serving as a drop-in replacement for MNIST digits.
        <strong>MobileNetV3</strong> is a lightweight convolutional neural network designed for efficiency on mobile and edge devices. In this project, 
        I fine-tuned the model by replacing the final classification layer to suit our 10 fashion categories and freezing the rest of the network. This 
        approach significantly reduces training time while leveraging powerful pre-learned image representations.
    </p>
    <p>
        Key Concepts covered: Transfer learning with pre-trained CNNs, Modifying classifier layers for custom tasks,
        Freezing and unfreezing network parameters, Resizing grayscale images to fit color CNN input formats, Efficient training with only a small subset of parameters,
        Model evaluation and visualization of classification results
    
    </p>
    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Transfer%20Learning%20using%20MobileNet/Transfer%20learning%20using%20MobileNetV3.ipynb">GitHub Link</a>
    </p>
</section>


        <section>
    <h2>Project 4: Prompt-Based Spam Email Classifier using a Foundation Model - Prompt Based</h2>
    <p>
        In this project, I built a spam email classifier using a <strong>foundation model</strong> accessed through prompting—without any training or fine-tuning. 
        The goal was to showcase how a pre-trained large language model (LLM) can be used "as-is" to solve classification problems through smart prompt engineering.
        I used the <code>sms_spam</code> dataset from HuggingFace, which contains real SMS messages labeled as SPAM or NOT SPAM. The model was prompted to classify a batch of messages based on their content, and its output was parsed and evaluated against ground truth labels.
    </p>
    <p>
        The project was carried out in two stages:
        <ul>
            <li><strong>Zero-shot classification:</strong> The LLM was asked to classify SMS messages with no prior examples.</li>
            <li><strong>Few-shot classification:</strong> The prompt was augmented with a few labeled examples before asking the model to classify new, unlabeled messages. This improved its accuracy significantly.</li>
        </ul>
    </p>

    <p>
        Key concepts covered: Loading and preparing a labeled dataset from HuggingFace, Constructing zero-shot and few-shot prompts for classification tasks,
            Parsing LLM JSON responses and evaluating prediction accuracy, Using LLMs for real-world tasks without fine-tuning
        
    </p>

    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Prompt%20Based%20Spam%20Email%20Classifier%20using%20a%20Foundation%20Model/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>


        <section>
    <h2>Project 5: Sentiment Classification Using DistilBERT on IMDB Dataset - Linear Probing</h2>
    <p>
        In this project, I fine-tuned a <strong>DistilBERT</strong> model (a lightweight version of BERT) using the 
        <a href="https://huggingface.co/datasets/imdb" target="_blank">IMDB movie review dataset</a> to classify movie reviews 
        as either <em>positive</em> or <em>negative</em>. This task showcases the power of <strong>transformer-based models</strong> 
        in real-world natural language processing (NLP) applications. I used the <strong>Hugging Face Transformers</strong> library to tokenize the dataset, set up the classification model, 
        and train it using the high-level <code>Trainer</code> API. To accelerate training and preserve compute, I froze the base 
        DistilBERT model and trained only the classification head. 
        This project demonstrates end-to-end fine-tuning of a foundation model using minimal code, while still maintaining flexibility 
        and transparency in each training step.
    </p>

    <p>
        Key steps included:
        
            Preprocessing the dataset using <code>AutoTokenizer</code> with padding and truncation,
            Loading a pre-trained <code>distilbert-base-uncased</code> model and configuring it for binary classification,
            Training using Hugging Face's <code>Trainer</code> and evaluating accuracy on the test set,
            Analyzing model predictions and identifying misclassified examples for error analysis
        
    </p>

    

    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Bert%20Sentiment%20Classifier%20-%20Linear%20Probing/BERT_Sentiment_Classifier.ipynb">GitHub Link</a>
    </p>
</section>



       <section>
    <h2>Project 6: Fine-Tuning DistilBERT for SMS Spam Classification - Full Fine Tuning</h2>
    <p>
        In this project, I built a binary text classifier by <strong>fully fine-tuning a DistilBERT model</strong> using the 
        <a href="https://huggingface.co/datasets/sms_spam" target="_blank">SMS Spam dataset</a> from Hugging Face. The task involved 
        classifying short text messages as either <em>spam</em> or <em>not spam</em>, a practical application of NLP in communication security.
        Unlike prompt-based or head-only training, this project involved **unfreezing all layers** of the pre-trained transformer model, 
        allowing the entire DistilBERT architecture to adapt to the task through backpropagation. This is known as <strong>full fine-tuning</strong>, 
        which typically results in better performance on downstream tasks with enough data. This project demonstrates a robust end-to-end training pipeline for a real-world classification task using transformer models.
        It also emphasizes the benefits and trade-offs of full fine-tuning compared to parameter-efficient alternatives.
    </p>

    

    <p>
        Key steps included:
        
            Loading and splitting the SMS Spam dataset into training and testing subsets,
            Tokenizing text data using the <code>AutoTokenizer</code> from Hugging Face,
            Fully fine-tuning <code>distilbert-base-uncased</code> using the <code>Trainer</code> API,
            Evaluating model accuracy on unseen test messages,
            Manually inspecting misclassified examples for qualitative error analysis
        
    </p>


    <p>
        <a href="https://github.com/Swatkats13/Projects/blob/main/Bert%20Spam%20Classifier%20-%20Full%20Fine%20Tuning/cleaned_notebook.ipynb">GitHub Link</a>
    </p>
</section>



    <section>
  <h2>Project 7: Lightweight Fine-Tuning of DistilBERT for SMS Spam Detection using LoRA</h2>
  <p>
    In this project, I implemented <strong>parameter-efficient fine-tuning (PEFT)</strong> using <strong>LoRA (Low-Rank Adaptation)</strong> on the 
    <a href="https://huggingface.co/datasets/sms_spam" target="_blank">SMS Spam</a> dataset to build a binary classifier that detects whether an SMS message is <em>spam</em> or <em>not spam</em>. 
    Unlike full fine-tuning where all model weights are updated, PEFT updates only a small subset of parameters—making it a lightweight and scalable strategy, especially in resource-constrained environments.
    The project highlights how <strong>DistilBERT</strong> can be effectively adapted to a new classification task using LoRA modules inserted into its transformer layers. 
    The model was evaluated before and after fine-tuning using the Hugging Face Trainer API to measure performance gains with minimal compute cost.
    After fine-tuning, the model’s accuracy improved significantly—from <strong>~81%</strong> to <strong>~98%</strong>—proving that PEFT via LoRA is both efficient and highly effective.

  </p>

   <p>
    <strong>Key Concepts Covered:</strong>
    
      Understanding the difference between full fine-tuning and parameter-efficient fine-tuning (PEFT),
      Applying <code>LoRA</code> (Low-Rank Adaptation) to reduce training overhead while retaining performance,
      Choosing target modules (<code>q_lin</code> and <code>v_lin</code>) in the transformer architecture for LoRA injection,
      Using the Hugging Face <code>Trainer</code> API for training and evaluation workflows,
      Measuring model performance before and after fine-tuning using accuracy as the evaluation metric
    
  </p>


  <p>
    <a href="https://github.com/Swatkats13/Projects/blob/main/LoRa%20based%20PEFT%20Spam%20Classifier/LoRA_based_PEFT_Spam_Classifier_.ipynb" target="_blank">GitHub Link</a>
  </p>
</section>



<section>
  <h2>Project 8: Vector Search from Scratch using NumPy</h2>
  <p>
    In this project, I implemented a basic <strong>vector search engine from scratch using only NumPy</strong>. The goal was to demystify how popular vector databases like FAISS or Pinecone work internally by building core functionality such as distance calculations and K-Nearest Neighbors (KNN) retrieval without using any external ML libraries.
    This project highlights the importance of <strong>vector similarity</strong> in machine learning and information retrieval. I implemented and compared both <strong>Euclidean distance</strong> and <strong>Cosine distance</strong>, and explored how <em>normalization of vectors</em> can affect distance calculations and the ranking of nearest neighbors.
    This hands-on implementation helps understand when to use each distance metric, and how raw vs normalized embeddings affect similarity comparisons—an essential concept in vector-based search and NLP systems.

  </p>
  

  <p>
    Key steps included:
      Implementing Euclidean and Cosine distance functions manually,
      Building a configurable K-Nearest Neighbors function with choice of distance metric,
      Testing all components with known inputs to ensure correctness,
      Analyzing how normalization affects nearest neighbor rankings,
      Visualizing and printing the query vector and its nearest neighbors
    
  </p>


  <p>
    <a href="https://github.com/Swatkats13/Projects/blob/main/Basic%20Vector%20Search/Basic_Vector_Search.ipynb">GitHub Link</a>
  </p>


<section>
  <h2>Project 9: Semantic Search on Rick and Morty Quotes using Sentence Transformers</h2>
  <p>
    This project demonstrates how to build a lightweight semantic search system using <strong>sentence embeddings</strong> generated by 
    <a href="https://www.sbert.net" target="_blank">Hugging Face Sentence Transformers</a>. By encoding Rick and Morty quotes into vector space, 
    we enable semantic similarity search: retrieving quotes most related to a given user query — even if the wording is different. <strong>Dataset:</strong> A curated list of humorous and philosophical quotes from the 
    <a href="https://parade.com/tv/rick-and-morty-quotes" target="_blank">Parade article on Rick and Morty quotes</a>. This project offers a simple and effective example of how embeddings can power semantic search across unstructured text.
 
  </p>

  <p><strong>Key concepts covered:</strong></p>
  
    How sentence embeddings represent textual meaning as vectors,
    Generating embeddings using <code>paraphrase-MiniLM-L6-v2</code>,
    Semantic similarity search using Euclidean distance over embeddings,
    Comparing raw and normalized embedding distances,
    Finding nearest neighbors from a list of quotes using numpy

  <p>
    <strong>Example query:</strong><br/>
    “Are you the cause of your parents' misery?” returns semantically similar quotes from Rick and Morty — even if phrased differently.
  </p>

  <p>
    <a href="https://github.com/Swatkats13/Projects/blob/main/Embedding%20Functions/Embedding_Functions.ipynb" target="_blank">GitHub Link</a>
  </p>
</section>


<section>
  <h2>Project 10: Vector Database Basics with LanceDB</h2>
  <p>
    In this project, I explored the core functionalities of <strong>LanceDB</strong>, a modern open-source vector database
    designed for high-performance retrieval in generative AI, recommendation systems, and semantic search applications.
    As a hands-on task, I created a toy dataset of cat and dog embeddings and queried it to find the most similar animals.
    I also generated and indexed a 100,000-vector dataset to simulate high-volume search scenarios and measure performance gains from ANN indexing.
    This project was a great introduction to building scalable AI systems that blend structured metadata with vector-based retrieval.

  </p>

  <p><strong>Key concepts covered:</strong></p>
  Understanding what vector databases are and why they are needed in modern AI pipelines,
    Creating and managing LanceDB tables with vectors and structured metadata,
    Performing <strong>vector search</strong> using both Euclidean and cosine distances,
    Filtering results with SQL-like conditions alongside vector queries,
    Building <strong>Approximate Nearest Neighbor (ANN)</strong> indices (IVFPQ) for fast retrieval on large datasets,
    Benchmarking query latency with and without ANN indexing,
    Versioning and rollback support for debugging data errors,
    Deleting records and managing table lifecycle operations
  

  
  <p>
    <a href="https://github.com/Swatkats13/Projects/blob/main/Vector%20Database%20Basics%20with%20LanceDB/Vector_Database_Basics_with_LanceDB.ipynb" target="_blank">GitHub Link</a>
  </p>
</section>

<section>
  <h2>Project 11: Movie Recommender System using LanceDB and Matrix Factorization</h2>
  <p>
    In this project, I built a simple yet effective <strong>movie recommendation engine</strong> using matrix factorization and 
    <strong>LanceDB</strong>, an open-source vector database. The recommender is based on collaborative filtering using 
    the <a href="https://grouplens.org/datasets/movielens/latest/" target="_blank">MovieLens Latest Small Dataset</a>, 
    which includes user ratings for 9,000 movies.  As a demo, I queried the system with popular titles like <em>"Moana (2016)"</em> and 
    <em>"Rogue One: A Star Wars Story (2016)"</em> and received reasonable recommendations based on user similarity.
    This project showcases the potential of vector databases in delivering <strong>personalized recommendations</strong> in real time.
  
  </p>

  <p><strong>Key concepts covered:</strong></p>
        Pivoting user-movie ratings into a sparse ratings matrix,
        Using <strong>Singular Value Decomposition (SVD)</strong> for matrix factorization to generate movie embeddings,
        Combining ratings data with metadata such as movie titles, genres, and IMDb IDs,
        Creating and populating a LanceDB table using <strong>PyDantic schema</strong>,
        Performing vector similarity search in LanceDB to recommend similar movies,
        Building a recommendation function that returns IMDb links for top suggestions

  <p>
    <a href="https://github.com/Swatkats13/Projects/blob/main/Movie%20Recommender%20using%20SVD/Movie_Recommender_System_using_LanceDB_and_Matrix_Factorization.ipynb" target="_blank">
       GitHub Link
    </a>
  </p>
</section>

<section>
  <h2>Project 12: Multimodal Image Search with CLIP and LanceDB</h2>
  <p>
    This project demonstrates a <strong>multimodal search engine</strong> that retrieves relevant images from a dataset using <em>natural language queries</em>. We leverage the powerful <a href="https://huggingface.co/openai/clip-vit-base-patch32" target="_blank">CLIP model</a> by OpenAI, which can embed both images and text into the same latent space, enabling cross-modal retrieval using semantic similarity.
  </p>

  <p>
    The model was fine-tuned using the <a href="https://huggingface.co/datasets/zh-plus/tiny-imagenet" target="_blank">Tiny ImageNet</a> dataset, where each image is encoded into a 512-dimensional vector. The search pipeline stores these embeddings in <strong>LanceDB</strong>, an open-source vector database, enabling efficient retrieval via vector similarity.
  </p>

  <p><strong>Key concepts covered:</strong></p>
  
    Using <code>CLIP</code> for joint image and text embeddings,
    Building a <strong>semantic search engine</strong> over image data,
    Creating and querying vector tables using <code>LanceDB</code>,
    Converting and storing image data efficiently in databases,
    Implementing cross-modal retrieval using cosine similarity
  

  <p>
    The final system allows users to type any descriptive text like <em>"a fish swimming"</em> or <em>"a yellow flower"</em>, and it returns the most semantically relevant images.
  </p>

  <p>
    <a href="https://github.com/Swatkats13/Projects/blob/main/Multimodal%20Search%20with%20CLIP%20and%20LanceDB/cleaned_notebook.ipynb" target="_blank">GitHub Link</a>
  </p>
</section>



       
    </main>
</body>
</html>
